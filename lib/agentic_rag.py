"""
Agentic RAG components for enhancing retrieval and answer generation.
These components can be integrated with the existing PDF indexer.
"""

import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field, asdict
import json
import re

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("agentic_rag.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("agentic_rag")

# Data models for structured information
@dataclass
class RetrievedChunk:
    """Represents a chunk of text retrieved from the vector store"""
    content: str
    file_name: str
    page: int
    paragraph: int
    similarity_score: float
    
    def to_dict(self):
        return asdict(self)

@dataclass
class SubQuery:
    """Represents a subquery generated by the agent"""
    text: str
    parent_query: str
    rationale: str
    
    def to_dict(self):
        return asdict(self)

@dataclass
class VerifiedInformation:
    """Represents information that has been verified"""
    content: str
    source: str
    verification_status: str  # "verified", "contradicted", "unverifiable"
    confidence: float
    
    def to_dict(self):
        return asdict(self)

@dataclass
class RAGResponse:
    """Structured response from the RAG system"""
    answer: str
    sources: List[Dict[str, Any]] = field(default_factory=list)
    confidence: float = 0.0
    reasoning: str = ""
    
    def to_dict(self):
        return asdict(self)

class QueryPlanner:
    """Plans how to decompose and process a user query"""
    
    def __init__(self, client):
        self.client = client
        
    def decompose_query(self, original_query: str) -> List[SubQuery]:
        """Decompose a complex query into simpler sub-queries"""
        system_prompt = """
You are an expert query decomposition system. Your job is to break down complex questions into 
simpler sub-questions that can be answered independently.

For each sub-question:
1. Make it self-contained and specific
2. Ensure it can be answered with factual information
3. Provide a clear rationale for why this sub-question helps answer the original query

Format your response as a valid JSON array of objects, each with:
- "text": the sub-question text
- "rationale": why this sub-question is important for the original query

Sub-questions should be arranged in a logical sequence to build knowledge incrementally.
"""
        
        user_prompt = f"""
Original query: {original_query}

Please break this down into 2-4 sub-questions that would help answer the original query comprehensively.
Return ONLY the JSON array without explanations or code formatting.
"""
        
        try:
            if hasattr(self.client, 'chat') and hasattr(self.client.chat, 'completions'):
                # New OpenAI client
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.2,
                    max_tokens=1000
                )
                content = response.choices[0].message.content
            else:
                # Legacy client
                response = self.client.ChatCompletion.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.2,
                    max_tokens=1000
                )
                content = response.choices[0].message.content
            
            # Parse the response
            sub_queries_data = json.loads(content)
            
            # Sometimes GPT returns with a root key, so handle both cases
            if "sub_questions" in sub_queries_data:
                sub_queries_data = sub_queries_data["sub_questions"]
            
            # Convert to SubQuery objects
            sub_queries = []
            for data in sub_queries_data:
                sub_queries.append(SubQuery(
                    text=data["text"],
                    parent_query=original_query,
                    rationale=data["rationale"]
                ))
            
            return sub_queries
        
        except Exception as e:
            logger.error(f"Error decomposing query: {str(e)}")
            # Fall back to just using the original query
            return [SubQuery(
                text=original_query,
                parent_query=original_query,
                rationale="Original query used directly due to decomposition error"
            )]

class InformationVerifier:
    """Verifies information retrieved from the vector store"""
    
    def __init__(self, client):
        self.client = client
    
    def verify_information(
        self, 
        query: str, 
        retrieved_chunks: List[RetrievedChunk]
    ) -> List[VerifiedInformation]:
        """Verify retrieved information for consistency and reliability"""
        # Skip verification if no chunks
        if not retrieved_chunks:
            return []
        
        system_prompt = """
You are an expert information verifier. Your job is to analyze text chunks retrieved from documents 
and determine if they contain reliable information relevant to the query.

For each piece of information:
1. Assess if it directly addresses the query
2. Check for internal contradictions
3. Evaluate the confidence level based on specificity and relevance

Format your findings as a valid JSON array of objects, each with:
- "content": a specific piece of factual information extracted from the chunks
- "source": identifier of the source chunk
- "verification_status": either "verified", "contradicted", or "unverifiable"
- "confidence": a float between 0 and 1 indicating confidence level
"""
        
        # Prepare chunks information
        chunks_text = "\n\n".join([
            f"CHUNK {i+1} (from {c.file_name}, Page {c.page}, Para {c.paragraph}):\n{c.content}"
            for i, c in enumerate(retrieved_chunks)
        ])
        
        user_prompt = f"""
Query: {query}

Retrieved text chunks:
{chunks_text}

Please analyze these chunks and extract verified information relevant to the query.
Return ONLY the JSON array without explanations or code formatting.
"""
        
        try:
            if hasattr(self.client, 'chat') and hasattr(self.client.chat, 'completions'):
                # New OpenAI client
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.2,
                    max_tokens=1500
                )
                content = response.choices[0].message.content
            else:
                # Legacy client
                response = self.client.ChatCompletion.create(
                    model="gpt-4o", 
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.2,
                    max_tokens=1500
                )
                content = response.choices[0].message.content
            
            # Parse the response
            verified_data = json.loads(content)
            
            # Sometimes GPT returns with a root key, so handle both cases
            if "verified_information" in verified_data:
                verified_data = verified_data["verified_information"]
            
            # Convert to VerifiedInformation objects
            verified_info = []
            for data in verified_data:
                verified_info.append(VerifiedInformation(
                    content=data["content"],
                    source=data["source"],
                    verification_status=data["verification_status"],
                    confidence=float(data["confidence"])
                ))
            
            return verified_info
        
        except Exception as e:
            logger.error(f"Error verifying information: {str(e)}")
            # Fall back to treating all chunks as unverified
            verified_info = []
            for i, chunk in enumerate(retrieved_chunks):
                verified_info.append(VerifiedInformation(
                    content=chunk.content,
                    source=f"{chunk.file_name}, Page {chunk.page}, Para {chunk.paragraph}",
                    verification_status="unverifiable",
                    confidence=0.5
                ))
            return verified_info

class AnswerSynthesizer:
    """Synthesizes verified information into a comprehensive answer"""
    
    def __init__(self, client):
        self.client = client
    
    def synthesize_answer(
        self, 
        query: str, 
        verified_info: List[VerifiedInformation],
        sub_queries: Optional[List[SubQuery]] = None
    ) -> RAGResponse:
        """Synthesize a comprehensive answer from verified information"""
        system_prompt = """
You are an expert knowledge synthesizer. Your job is to create comprehensive answers to questions based 
on verified information retrieved from documents.

Your answer should:
1. Be factual and based ONLY on the provided information
2. Clearly indicate the confidence level and any uncertainties
3. Cite specific sources for key claims
4. Be concise but complete

Format your response with these sections:
1. A direct answer to the query
2. A list of sources with page numbers
3. An optional explanation of confidence level or limitations
"""
        
        # Prepare verified information
        verified_text = "\n\n".join([
            f"INFO {i+1} (Source: {info.source}, Status: {info.verification_status}, Confidence: {info.confidence}):\n{info.content}"
            for i, info in enumerate(verified_info)
        ])
        
        # Include sub-queries context if available
        sub_queries_text = ""
        if sub_queries:
            sub_queries_text = "\n\n".join([
                f"Sub-query {i+1}: {sq.text}\nRationale: {sq.rationale}"
                for i, sq in enumerate(sub_queries)
            ])
            sub_queries_text = f"\nThis query was broken down into the following sub-queries:\n{sub_queries_text}\n"
        
        user_prompt = f"""
Query: {query}{sub_queries_text}

Verified information from documents:
{verified_text}

Please synthesize this information into a comprehensive answer to the query.
Include relevant sources, and indicate confidence level.
"""
        
        try:
            if hasattr(self.client, 'chat') and hasattr(self.client.chat, 'completions'):
                # New OpenAI client
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1500
                )
                answer_text = response.choices[0].message.content
            else:
                # Legacy client
                response = self.client.ChatCompletion.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1500
                )
                answer_text = response.choices[0].message.content
            
            # Extract sources from the answer (simple heuristic)
            sources = []
            confidence = 0.0
            
            # Parse sources section from the response
            sources_section_match = re.search(r'sources:\s*(.+?)(?:\n\n|$)', answer_text, re.IGNORECASE | re.DOTALL)
            if sources_section_match:
                sources_text = sources_section_match.group(1)
                source_matches = re.findall(r'([^,]+, Page \d+(?:, Para \d+)?)', sources_text)
                for source in source_matches:
                    source_parts = source.split(', Page ')
                    if len(source_parts) >= 2:
                        file_name = source_parts[0].strip()
                        page_info = source_parts[1].strip()
                        page_num = int(re.search(r'\d+', page_info).group())
                        sources.append({
                            "file_name": file_name,
                            "page": page_num
                        })
            
            # Extract confidence from verified information
            if verified_info:
                # Calculate average confidence from verified items
                confidences = [info.confidence for info in verified_info if info.verification_status == "verified"]
                if confidences:
                    confidence = sum(confidences) / len(confidences)
                else:
                    confidence = 0.3  # Low confidence if no verified information
            
            # Create structured response
            rag_response = RAGResponse(
                answer=answer_text,
                sources=sources,
                confidence=confidence,
                reasoning="Based on verified information from the documents"
            )
            
            return rag_response
        
        except Exception as e:
            logger.error(f"Error synthesizing answer: {str(e)}")
            # Fall back to a simple response
            return RAGResponse(
                answer=f"I couldn't generate a complete answer to '{query}' due to processing errors. Please try rephrasing your question.",
                sources=[],
                confidence=0.0,
                reasoning="Error occurred during answer synthesis"
            )